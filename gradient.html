<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no" />

    <title>Gradient Descent</title>

    <script src="js/jquery-3.1.1.min.js"></script>
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link href="css/materialize.css" type="text/css" rel="stylesheet" media="screen,projection" />
    <link href="https://fonts.googleapis.com/css?family=Raleway|Satisfy" rel="stylesheet">
    <link href="css/style.css" type="text/css" rel="stylesheet" media="screen,projection" />
    <link rel="stylesheet" href="//code.jquery.com/ui/1.12.1/themes/base/jquery-ui.css">
    <script src="https://code.jquery.com/jquery-1.12.4.js"></script>

</head>

<body>
    <nav>
        <li class="nav-wrapper back-home" id="head">
            <a href="http://www.mecbar.com/" class="brand-logo mecbar">Mecbar</a>
            <a href="#" data-activates="mobile-demo" class="button-collapse"><i class="material-icons">menu</i></a>

            <ul class="right hide-on-med-and-down">
                <!--  <li> <a class="btn" onclick="Materialize.toast('Hello', 4000, 'Ciao' ,4000 )">Touch Me</a> </li>
              -->

                <li><a href="http://www.mecbar.com/#Ablog">Blog</a></li>
                <li><a href="http://www.mecbar.com/#Ablock">Blockchain</a></li>
                <li><a href="http://www.mecbar.com/#Amachine">Machine Learning</a></li>
                <li><a href="http://www.mecbar.com/#Alinux">Linux</a></li>
                <li><a href="http://www.mecbar.com/#Aus">Contatti</a></li>
                <li>
                    <a href=""> </a>
                </li>
                <li>
                    <a href=""> </a>
                </li>
            </ul>
            <ul class="side-nav" id="mobile-demo">
                <li><a href="http://www.mecbar.com/#Ablog">Blog</a></li>
                <li><a href="http://www.mecbar.com/#Ablock">Blockchain</a></li>
                <li><a href="http://www.mecbar.com/#Amachine">Machine Learning</a></li>
                <li><a href="http://www.mecbar.com/#Alinux">Linux</a></li>
                <li><a href="http://www.mecbar.com/#Aus">Contatti</a></li>
            </ul>
            </li>
    </nav>
    <br><br>
    <div class="row">
        <div class="col s12 m12 l12">
            <div class="logistic-titolo center spazio2001"> The Backpropagation with Gradient Descent </div>
        </div>
    </div>
    <div class="row">
        <div class="col s12 m12 l12 blocco">
            <b>Introduction </b>
        </div>
    </div>
    <div class="row">
        <div class="col l2"></div>
        <div class="col s12 m12 l8 blocco">
            <br> Backpropagation is a backward propagation of errors and is a powerful tool of the deep learning. With the Gradient Descent the backpropagation reduce the cost function and the time of execution. We now talk about of calculate the Gradient
            Descent.
        </div>
    </div>
    <div class="row">
        <div class="col s12 m12 l12 blocco">
            <b>Gradient Descent</b>
        </div>

        <div class="col s12 m12 l6 blocco">
            With the Gradient Descent we want find the weights that minimize the errors, the cost function, through some iterations for search the minimum. There are some method, we see the principal, the<b> Batch Gradient Descent</b>, the<b> Stochastic Gradient Descent</b>            and <b>Mini-Batch Gradient Descent</b>. The first, the Batch Gradient Descent, is a deterministic method that start always with the same data and produce the same outcome. It calculate the cost function of all the input data and then update
            the weights through the Backpropagation. This process is very expensive in time and resources for load all data in memory, especially as data gets big, for found the best cost function. The Stochastic Gradient Descent, shortened SGD, is a
            stochastic method because the outcome is not always the same. With SGD calculate the cost function of 1 input data and then upgrade the weights. This for every input data. This method is faster because not need of very resources and its most
            used when we have very input data.
            <br> The Mini-Batch Gradient Descent is recent and its balance between the first 2 method. We get derivative for a ”small” set of points, tipical mini batch size is 16 or 32, then update the weights and backpropagate it.
            <br> An Epoch refers to a single forward pass through all of the training data:<br> - in <b> Batch Gradient Descent</b> there is 1 step for epoch <br> - in <b> Stochastic Gradient Descent</b> there are n steps for epoch were n is the training
            set size<br> - in <b>Mini-Batch
            Gradient Descent</b> there are n steps for epoch were n is equal to training set size/batch-size(16 or 32)<br> Naturally <b>epoch</b> is another important parameter for Artificial Neural Network.<br>
            <img src="foto/gradient/steps.png" class="card-panel hoverable" width="100%" height="300px" alt="the epoch">

            <br> For prevent overfitting there are some tecnics call Optimizer. One of this is DROPOUT and most popular are ADAM and RMSPROP. The optimizer are variants to update the weights to give a better performance of Neural Network. RMSProp is a
            adaptive learning rate method is a variant of Adagrad method for upgrade the weights Its modulates the learning rate of each weight in base of their gradient value equalizing the effect.
            <br> Adam is a new method and most utilised. Its a variant of RMSProp. With Dropout we avoid overfitting reducing the number of nodes setting to 0 the value of nodes selected randomly. So the hidden layers are in lower number respect the input
            layers based to dropout parameter inserted into the Neural Network.
            <br>
            <img src="foto/gradient/dropout.png" class="card-panel hoverable" width="100%" height="300px" alt="the dropout graph">
        </div>


        <div class="col s12 m12 l6 blocco">
            <div class="card-panel hoverable">
                <img src="foto/gradient/gradient_graph.png" width="100%" height="300px" alt="the gradient graph">
                <br>
            </div>
            <div class="card-panel hoverable">
                <img src="foto/gradient/2d_search.png" width="100%" height="300px" alt="the 2d graph">
                <br>
            </div>
            <div class="card-panel hoverable">
                <img src="foto/gradient/3d_search.png" width="100%" height="100%" alt="the 3d graph">
            </div>

            <br>In the above image the graph of search of minimum cost function from 2 to 3 dimensional space.


            <br>The formula for calculate the cost function for every input data and for all the input data:<br>
            <br>
            <img class="card-panel hoverable" src="foto/gradient/cost_function2.png" width="100%" height="350px" alt="the cost function">

        </div>
    </div>


    <div class="row">
        <div class="col s12 m12 l12 blocco">
            <b>Example of calculation of the cost function </b>
        </div>
    </div>
    <div class="row">
        <div class="col s12 m12 l6">
            <div class="card-panel hoverable" style="height:400px">
                <img src="foto/gradient/calculate_c.png" width="100%" height="100%" alt="the cost function">
            </div>
            <div class="card-panel hoverable" style="height:400px">
                <img src="foto/gradient/cf_sgd.png" width="100%" height="100%" alt="the cost function">

            </div>
        </div>
        <div class="col s12 m12 l6 blocco">
            In this section we calculate the cost function of a simple linear regression function <code> Y = aX + b </code> with the Adjusted R-Squared method.<br> The Adjusted R-Squared is a method for calculate the error of the predictions value of
            a regression compared to the actuel value. Its Adjusted because the squared value is always positive so its divided for 2.<br> In this example the value of coefficients with a minimum cost function is <code>a = 1 and b = 1 </code> and the
            model for the regression is
            <code> y = x + 1 </code>. <br> In this example we simulated three cases with the coefficients a and b (1,1), (2,1) and (2,2). The results is in the column P1, P2 and P3. Then we calculate their cost function in column C1, C2 and C3. Doing
            the sum we have the total cost function and we take the coefficients with the minimum cost function. if we use the Backpropagation we take a part of the errors to select the Learning Rate that is a parameter very important of the Machine Learning.
            We select a very small Learning Rate (max 0.05) and update the weights.<br> Here in the schema we show the difference between the Batch Gradient Descent and the Stochastic Gradient Descent. In the SGD after calculate even cost function we
            update the weights while in the Batch we update the weights only after we calculate the total cost function. In Mini-Batch Gradient Descent we divide the training data into block with the batch size dimension. After we calcolate the cost function
            of one block and upgrade the weights.<br>


        </div>
    </div>

    <div class="row">
        <div class="col s12 m12 l12 blocco">
            <b>Computational Graph</b>
        </div>
    </div>

    <div class="row">
        <div class="col s12 m12 l6 card-panel hoverable">
            <img src="foto/gradient/comp_graph.png" width="100%" height="380px" alt="the computational graph">

        </div>
        <div class="col s12 m12 l6 blocco">
            For calculate the Gradient we introduce the Computational Graph. The Computational Graph is a method for represent a process in some steps, a data flow graph, were represent the operation in the chart. Each step corresponds to a simple operation. There
            are some inputs and produces some output as a function. In the image the Computational Graph. The calculation of the local gradient is influenced by the upstream gradient in every step into the Backward Pass process until the final cost function.<br>            The gradient calculated come back to the hidden layers and then upgrade the weights and the process restart with the Forward Pass.
        </div>
    </div>

    <div class="row">
        <div class="col s12 m12 l12 blocco">
            <b>Example 1</b>
        </div>
    </div>

    <div class="row">
        <div class="col l2"></div>
        <div class="col s12 m12 l8">
            <svg height="5" width="100%">
  <line x1="0" y1="0" x2="100%" y2="0" style="stroke:rgba(27, 7, 209, 0.863);stroke-width:3" />
</svg>
        </div>
    </div>
    <div class="row">
        <div class="col l2"></div>
        <div class="col s12 m12 l8 blocco">
            The Computational Graph is used for FeedForward and for BackForward. Now a simple example of a Computational Graph that represent a function. We calculate the value of the function in black and the gradient in red with the rules show in the image.<br>            The function
            <b>f(x,y,z) = f(x+y)z with  x = 3, y = -4, z = 7 and we insert w = (x + y) </b>

        </div>
    </div>
    <div class="row">
        <div class="col l2"></div>
        <div class="col s12 m12 l8">

            <img src="foto/gradient/gradient_descent.png" width="100%" height="350px" alt="gradient">
            <svg height="5" width="100%">
    <line x1="0" y1="0" x2="100%" y2="0" style="stroke:rgba(27, 7, 209, 0.863);stroke-width:3" />
</svg>
        </div>
    </div>
    <div class="row">
        <div class="col s12 m12 l12 blocco">
            <b>Local Gradient</b>
        </div>
    </div>


    <div class="row">
        <div class="col s12 m12 l6 spazio2001">
            <img src="foto/gradient/gradient.png" width="100%" height="350px" alt="gradient">

        </div>

        <div class="col s12 m12 l6 blocco">

            In the graph we can see how calculate the gradients with the local gradient and the Chain Rule. The Chain Rule tells us how find the derivative of a composite function.
            </br>The <b>MULTIPLY gate</b> return us the gradient with the following rule:<br> - the gradient of x is equal to upstream gradient multiply by y value in the Forward Pass <br>- the gradient of y is equal to upstream gradient multiply by x
            value in the Forward Pass. <br> The <b>ADD gate</b> return us the same gradient value of upstream gradient to x and y gradient. <br> The<b> MAX gate</b> return us the gradient with the following rule: <br> - who has the greater value in Forward
            Pass between x and y take the gradient equal to the upstream and the other take the value equal to zero.


        </div>
    </div>

    <div class="row">
        <div class="col s12 m12 l12 blocco">
            <b>Upgrade the Weights</b>
        </div>
        <div class="col l2"></div>
        <div class="col s12 m12 l8 spazio2001">
            Now we have calculate the gradient and in this example we upgrade the weights with the following formula: <br>
            <code> w_new = w_old - &alpha; * derivative</code>
            <br> were w_old is the old weights, &alpha; is Learning Rate and derivative is the partial derivative(gradient) of every input layers
            <br> w_old are w1_old = 2, w2_old = 3, w3_old = 1 and learning_rate = 0.05<br>
            <code>
    w1_new = 2 - (0.05*7) <br>
    w2_new = 3 - (0.05*7)  <br>
    w3_new = 2 - (0.05*-1) <br>

    Now we put the new weigths in another iteration while we fit our model. 
</code>
        </div>
    </div>

    <div class="row">
        <svg height="5" width="100%">
  <line x1="0" y1="0" x2="100%" y2="0"style="stroke:rgba(27, 7, 209, 0.863);stroke-width:3" />
</svg>
    </div>
    <div class="row">
        <div class="col s12 m12 l12 blocco">
            <b>Example 2</b>
        </div>
    </div>
    <div class="row">
        <div class="col l2"></div>
        <div class="col s12 m12 l8 blocco spazio2001">
            Now another example of Computational Graph for calculate the Forward Pass and the Backward Pass for a Neural Network. In graph the formula of this example is the Sigmoid function <img src="foto/gradient/sigmoid.png" width="150px" height="30px"
                alt="sigmoid"> of X were X is equal of function of the regression y = (&beta; <span class="mat1"> 0</span> + &beta;<span class="mat1"> 1</span>x<span class="mat1"> 1</span> + &beta;<span class="mat1"> 2</span>x
            <span class="mat1"> 2</span> + ... + &beta;
            <span class="mat1"> n</span>x<span class="mat1"> n</span>) <br> In practice we represent a logistic regression how you can see in the project of the <a href="progetto_banka.html" target="_blank">Bank Marketing</a>. Here we have only 2 input
            layer X1 and X2 but the process for calculate the Backpropagation is the same. In the graph in red the value of gradient from right to left that with the Backpropagation we update the weights of Neural Network.
        </div>
    </div>
    <div class="row">
        <div class="col l2"></div>
        <div class="col s12 m12 l8">
            <svg height="5" width="100%">
  <line x1="0" y1="0" x2="100%" y2="0" style="stroke:rgba(27, 7, 209, 0.863);stroke-width:2" />
</svg>
            <img src="foto/gradient/gradient_exa.png" width="100%" height="350px" alt="gradient example">
            <svg height="5" width="100%">
  <line x1="0" y1="0" x2="100%" y2="0" style="stroke:rgba(27, 7, 209, 0.863);stroke-width:3" />
</svg>
        </div>
    </div>


</body>

</html>

</html>